{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: The Jordan Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "## Exercise 1: Initializing the network\n",
    "Consider the Jordan network (with $f(x) = \\tanh(x) = (e^x - e^{-x})(e^x + e^{-x})^{-1}$ and $\\varphi(x) = \\sigma(x) = (1+e^{-x})^{-1}$ and transposed weight matrices compared to the lecture notes)\n",
    "$$\n",
    "s(t) = W x(t) + R \\hat y(t-1) \\\\\n",
    "a(t) = \\tanh(s(t)) \\\\\n",
    "z(t) = V a(t) \\\\\n",
    "\\hat y(t) = \\sigma(z(t))\n",
    "$$\n",
    "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are matrices of appropriate sizes and $\\hat y(0) = 0$. \n",
    "\n",
    "Write a function `init` that takes a `model` and integers $D, I, K$ as arguments and stores the matrices $W, R, V$ as members `model.W`, `model.R`, `model.V`, respectively. The matrices should be `numpy` arrays of appropriate sizes and filled with random values that are uniformly distributed between -0.01 and 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import random \n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "class Obj(object):\n",
    "    pass\n",
    "\n",
    "model = Obj()\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "\n",
    "#W [IxD] dimension of current layer is s & prev layer (x) but .T to be @x, W@x\n",
    "#R [IxK] current layer is s & prev is y hat\n",
    "#V [KxI] current layer is z & prev is a\n",
    "\n",
    "def init(model, D, I, K,T):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    model.W = np.random.uniform(low=-0.01, high=0.01, size=(I,D))\n",
    "    model.R = np.random.uniform(low=-0.01, high=0.01, size=(I,K))\n",
    "    model.V = np.random.uniform(low=-0.01, high=0.01, size=(K,I))\n",
    "    \n",
    "\n",
    "Obj.init = init \n",
    "model.init(D, I, K) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Numerical stability of the binary cross-entropy loss function\n",
    "\n",
    "We will use the binary cross-entropy loss function to train our RNN, which is defined as \n",
    "$$\n",
    "L(\\hat y, y) = -y \\log \\hat y - (1-y) \\log (1-\\hat y) \\quad \\text{where} \\quad \\hat y = \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "is the sigmoid function. Its argument $z$ is called *logit*. For reasons of numerical stability it is better to let the model emit the logit and incorporate the sigmoid function into the loss function. Explain why this is the case and how we can gain numerical stability by combining the two functions into one. *Hint:* Find a numerically stable version of the function $f(z) = \\log(1+e^{z})$ and rewrite $L(z,y)$ in terms of $f(z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "z is an unbounded linear function that when passed by the sigmoid function is bounded by [0,1].\n",
    "\n",
    "$$e^{-z} \\text {must be emitted from the model since if:}$$\n",
    "$$z << 0 \\rightarrow y \\approx 0 \\rightarrow log 0 = \\infty$$\n",
    "\n",
    "So, it is more stable to combine operations into 1 layer to take advantage of the log-sum-exp:\n",
    "\n",
    "$$log \\sum_{t=1}^{T} e^z = a + log \\sum_{t=1}^{T} e^{z+a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The forward pass\n",
    "Write a function `forward` that takes a `model`, a sequence of input vectors $(x(t))_{t=1}^T$, and a label `y` as arguments. The inputs will be represented as a `numpy` array of shape `(T, D)`. It should execute the behavior of the Jordan network and evaluate the numerically stablilized binary cross-entropy loss at the end of the sequence and return the resulting loss value. Store the sequence of hidden activations $(a(t))_{t=1}^T$ and the sequence of logits $(z(t))_{t=1}^T$ into `model.a` and `model.z`, respectively, using the same representation scheme as for the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.matlib\n",
    "\n",
    "def forward(self, x, y):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    self.s = np.zeros((T,I))\n",
    "    self.a = np.zeros((T,I))\n",
    "    self.z = np.zeros((T,K))\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    self.y_hat = np.zeros((T+1,K))\n",
    "    prev_y_hat=0\n",
    "    #loss = 0\n",
    "    \n",
    "    for t in range(0,T):\n",
    "        s = (self.W @ x[t]) + (self.R @ self.y_hat[t-1])\n",
    "        self.s[t] = s\n",
    "        a = np.tanh(s)\n",
    "        self.a[t] = a\n",
    "        z = self.V @ a\n",
    "        self.z[t] = z\n",
    "        y_hat = sigmoid(z)\n",
    "        self.y_hat[t] = y_hat\n",
    "        prev_y_hat=y_hat\n",
    "    loss = - y*np.log(y_hat) - (1-y)*np.log(1-y_hat)\n",
    "        \n",
    "    loss /= T\n",
    "    return loss     \n",
    "\n",
    "Obj.forward = forward\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: The computational graph\n",
    "\n",
    "Visualize the functional graph of the Jordan network unfolded in time. The graph should show at least 3 consecutive time steps. Use the package `networkx` in combination with `matplotlib` to draw a directed graph with labelled nodes and edges. If you need help take a look at [this guide](https://networkx.guide/visualization/basics/). Make sure to arrange the nodes in a meaningful way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_node(\"y_hat(0)\",pos=(0,0))\n",
    "G.add_node(\"y_hat(1)\",pos=(3,-2))\n",
    "G.add_node(\"y_hat(2)\",pos=(5,-2))\n",
    "G.add_node(\"y_hat(3)\",pos=(7,-2))\n",
    "\n",
    "G.add_node(\"a(1)\",pos=(3,0))\n",
    "G.add_node(\"a(2)\",pos=(5,0))\n",
    "G.add_node(\"a(3)\",pos=(7,0))\n",
    "\n",
    "G.add_node(\"x(1)\",pos=(3,2))\n",
    "G.add_node(\"x(2)\",pos=(5,2))\n",
    "G.add_node(\"x(3)\",pos=(7,2))\n",
    "\n",
    "G.add_edge(\"y_hat(0)\", \"a(1)\", weight=\"R\")\n",
    "G.add_edge(\"a(1)\", \"y_hat(1)\", weight=\"V\")\n",
    "G.add_edge(\"y_hat(1)\", \"a(2)\", weight=\"R\")\n",
    "G.add_edge(\"a(2)\", \"y_hat(2)\", weight=\"V\")\n",
    "G.add_edge(\"y_hat(2)\", \"a(3)\", weight=\"R\")\n",
    "G.add_edge(\"a(3)\", \"y_hat(3)\", weight=\"V\")\n",
    "G.add_edge(\"x(1)\", \"a(1)\", weight=\"W\")\n",
    "G.add_edge(\"x(2)\", \"a(2)\", weight=\"W\")\n",
    "G.add_edge(\"x(3)\", \"a(3)\", weight=\"W\")\n",
    "\n",
    "labels = nx.get_edge_attributes(G, 'weight')\n",
    "positions = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "pos = nx.planar_layout(G)\n",
    "\n",
    "nx.draw_networkx(G, with_labels = True, pos=positions, node_size=900)\n",
    "nx.draw_networkx_edges(G, pos= positions, edgelist=G.edges())\n",
    "nx.draw_networkx_edge_labels(G, pos= positions, edge_labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Derivative of the loss\n",
    "\n",
    "Calculate the derivative of the binary cross-entropy loss function $L(z, y)$ with respect to the logit $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "$$\n",
    "L(\\hat y, y) = -y \\log \\hat y - (1-y) \\log (1-\\hat y) \\quad $$\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial L} { \\partial \\hat{y}} = \\frac{-y}{\\hat{y}}  \\hat{y}' + \\frac{-(1-y)}{1 - \\hat{y}} (1 - \\hat{y})'}$$\n",
    "\n",
    "$$= \\frac{-y}{\\hat{y}} \\hat{y}' + \\frac{1 - y}{1 - \\hat{y}} (1 - \\hat{y})'$$\n",
    "\n",
    "$$= \\big( \\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} \\big) \\hat{y}'$$\n",
    "\n",
    "$$= \\big( \\frac{-y + y\\hat{y} + \\hat{y} - y\\hat{y}}{\\hat{y}(1 - \\hat{y})} \\big) \\hat{y}'$$\n",
    "\n",
    "$$= \\big( \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})} \\big) \\hat{y}' $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z} = \\sigma(z) (1 - \\sigma(z)) \\frac{\\sigma(z) - y}{\\sigma(z)(1 - \\sigma(z))}$$\n",
    "\n",
    "$$= \\sigma(z) - y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Gradients with respect to network parameters\n",
    "Compute gradients for the weights of the Jordan network analytically. That is, derive backpropagation through time for the Jordan network. To do this, it is crucial to first find the derivative w.r.t. the network outputs $\\psi^\\top(t) = \\partial L / \\partial z(t)$ and hidden pre-activations $\\delta^\\top(t) = \\partial L / \\partial s(t)$. We use the shorthand notations $L = \\sum_{t=1}^T L(t)$ and $L(t) = L(y(t), \\hat y(t))$ for convenience. We use numerator-layout notation for partial derivatives (like in the lecture notes) which lets us multiply the chain rule from left to right. \n",
    "\n",
    "Compute the gradients $\\psi^\\top(t), \\delta^\\top(t), \\nabla_W L, \\nabla_R L, \\nabla_V L$. *Hint:* Take a look at the  graph from the previous exercise to see the functional dependencies. \n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "1. $\\psi^\\top(t) = \\frac{\\partial {L}}{\\partial{z(t)}} = \\frac{\\partial {L}}{\\partial{\\hat y(t)}} . \\frac{\\partial {\\hat y(t)}}{\\partial{z(t)}} = \\hat y - y $\n",
    "\n",
    "\n",
    "        \n",
    "2. $\\delta^\\top(t)^T = \\frac{\\partial L}{\\partial s(t)} = \\frac{\\partial {L}}{\\partial{\\hat y(t)}} . \\frac{\\partial {\\hat y(t)}}{\\partial{z(t)}} . \\frac{\\partial z(t)}{\\partial a(t)} . \\frac{\\partial a(t)}{\\partial s(t)} = \\Big[ \\frac{\\partial {L}}{\\partial{\\hat y(t)}} + \\frac{\\partial{L}}{\\partial{s(t+1)}}\\frac{\\partial{s(t+1)}}{\\partial{\\hat y(t)}} \\Big] . \\frac{\\partial {\\hat y(t)}}{\\partial{z(t)}} . \\frac{\\partial z(t)}{\\partial a(t)} . \\frac{\\partial a(t)}{\\partial s(t)}= (\\psi^\\top(t) + \\delta(t+1) .R) . V (1-tanh^2s(t)) $\n",
    "$= \\psi^\\top(t) . V (1-a(t)^2)$\n",
    "    \n",
    "    \n",
    "    \n",
    "3. $\\nabla_W L$ = ${\\frac{\\partial L}{\\partial r}}^T = \\sum {\\frac{\\partial L}{\\partial s(t)}} . {\\frac{\\partial s(t)}{\\partial W}} = \\sum {\\delta(t) x(t)}^T $\n",
    "\n",
    "\n",
    "    \n",
    "4. $\\nabla_R L$ = ${\\frac{\\partial L}{\\partial w}}^T =  \\sum {\\frac{\\partial L}{\\partial s(t)}} . {\\frac{\\partial s(t)}\n",
    "{\\partial R}} = \\sum {\\delta(t) \\hat{y}(t-1)}^T $\n",
    "\n",
    "\n",
    "\n",
    "5. $\\nabla_V L$ = ${\\frac{\\partial L}{\\partial v}}^T = \\sum {\\frac{\\partial L}{\\partial z(t)}} . {\\frac{\\partial z(t)}{\\partial V}} = \\sum {\\psi(t) a(t)}^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: The backward pass\n",
    "Write a function `backward` that takes a model `self` as argument. The function should compute the gradients of the loss with respect to all model parameters and store them to `model.dW`, `model.dR`, `model.dV`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    y = self.y\n",
    "    x = self.x\n",
    "    R = self.R\n",
    "    V = self.V\n",
    "    \n",
    "    self.dW = 0\n",
    "    self.dR = 0\n",
    "    self.dV = 0\n",
    "    self.delta = np.zeros((T,I))\n",
    "    \n",
    "    for t in range(T-1, -1, -1):\n",
    "        psi = (self.y_hat[t] - self.y)\n",
    "        \n",
    "        self.delta[t] = (psi+ self.delta[t-1] @ self.R) @ self.V @ np.diag(1-(self.a[t])**2)\n",
    "            \n",
    "        self.dW += np.outer(self.delta[t], self.x[t].T)\n",
    "        self.dR += np.outer(self.delta[t], self.y_hat[t-1].T)\n",
    "        self.dV += np.outer(psi, self.a[t].T)\n",
    "\n",
    "Obj.backward = backward\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Gradient checking\n",
    "Write a function `grad_check` that takes a model `self`, a float `eps` and another float `thresh` as arguments and computes the numerical gradients of the model parameters according to the approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2 \\varepsilon}.\n",
    "$$\n",
    "If any of the analytical gradients are farther than `thresh` away from the numerical gradients the function should throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(self, eps, thresh):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    W = self.W\n",
    "    R = self.R\n",
    "    V = self.V\n",
    "    \n",
    "    weights = [[W,'W'],[R,'R'],[V,'V']]\n",
    "    \n",
    "    def check_sign(m, name):\n",
    "        m_grad_plus = np.zeros(m.shape)\n",
    "        m_grad_minus = np.zeros(m.shape)\n",
    "        \n",
    "        for sign in (1,-1):\n",
    "            for i in range (0, m.shape[0]):\n",
    "                for j in range (0, m.shape[1]):\n",
    "                    m_eps = np.copy(m)\n",
    "                    m_eps[i,j] += sign*eps\n",
    "                \n",
    "                    if(name=='W'):\n",
    "                        model.W = np.copy(m_eps)\n",
    "                    elif (name=='R'):\n",
    "                        model.R = np.copy(m_eps)\n",
    "                    elif (name=='V'):\n",
    "                        model.V = np.copy(m_eps)\n",
    "                    \n",
    "                    if(sign==1):\n",
    "                        m_grad_plus[i,j] = self.forward(self.x,self.y)\n",
    "                    elif(sign==-1):\n",
    "                        m_grad_minus[i,j] = self.forward(self.x,self.y)\n",
    "        \n",
    "        num_grad = (m_grad_plus - m_grad_minus)/(2*eps)\n",
    "        \n",
    "        if(name=='W'):\n",
    "            real_grad = self.dW\n",
    "        elif (name=='R'):\n",
    "            real_grad = self.dR\n",
    "        elif (name=='V'):\n",
    "            real_grad = self.dV\n",
    "        \n",
    "        diff = np.max(num_grad - real_grad)\n",
    "       # num= np.linalg.norm(num_grad - real_grad)\n",
    "       # den= np.linalg.norm(real_grad) + np.linalg.norm(num_grad)\n",
    "       # diff=num/den\n",
    "        return diff\n",
    "    \n",
    "    for w,name in weights:\n",
    "        diff = check_sign(w,name)\n",
    "        \n",
    "        if (diff<thresh):\n",
    "            print(\"Grad check for: \" +name+ \" passed:\", diff)\n",
    "        else:\n",
    "            print(\"Grad check for: \" +name+ \" failed:\", diff)\n",
    "                    \n",
    "\n",
    "Obj.grad_check = grad_check\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Parameter update\n",
    "\n",
    "Write a function `update` that takes a `model` and a float argument `eta`, which represents the learning rate. The method should implement the gradient descent update rule $\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L$ for all model parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    for t in range(0,T):\n",
    "        self.R = self.R - eta*self.dR\n",
    "        self.V = self.V - eta*self.dV \n",
    "        self.W = self.W - eta*self.dW\n",
    "\n",
    "Obj.update = update\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10: Data generation\n",
    "\n",
    "There are two classes, both occurring with probability 0.5. There is one input unit. Only the first sequence element conveys relevant information about the class. Sequence elements at positions $t > 1$ are generated by a Gaussian with mean zero and variance 0.2. The first sequence element is 1.0 (-1.0) for class 1 (2). Target at sequence end is 1.0 (0.0) for class 1 (2)\n",
    "\n",
    "Write a function `generate_data` that takes an integer `T` as argument which represents the sequence length. Seed the `numpy` random generator with the number `0xDEADBEEF`. Implement the Python generator pattern and produce data in the way described above. The input sequences should have shape `(T, 1)` and the target values should have shape `(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0xDEADBEEF)\n",
    "\n",
    "def generate_data(T):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    data = np.zeros((T,1),dtype=float)\n",
    "    \n",
    "    for i in range(1,T):\n",
    "        data[i] = np.random.normal(0, np.sqrt(0.2))\n",
    "    \n",
    "    values=[-1.0,1.0]\n",
    "    P=[0.5,0.5]\n",
    "    data[0]=np.random.choice(values,replace=False, p=P)\n",
    "\n",
    "    if data[0]==1.0:\n",
    "        y=1.0\n",
    "    elif data[0]==-1.0:\n",
    "        y=0.0\n",
    "    \n",
    "    target = np.array([y])\n",
    "        \n",
    "    #print(data.T,target)\n",
    "    return (data,target)\n",
    "\n",
    "data = generate_data(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11: Network training\n",
    "\n",
    "Train a Jordan network with 32 hidden units. Start with input sequences of length one and tune the leraning rate and the number of update steps. Then increase the sequence length by one and tune the hyperparameters again. What is the maximal sequence length for which the Jordan network can achieve a performance that is better than random? Visualize your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "def train(self, epochs=1, batchsize=1, eta=0.001,sequenceLen=1):\n",
    "    losses=[]\n",
    "    for i in range(epochs):\n",
    "        loss=0\n",
    "        for j in range(batchsize):\n",
    "            xx,y=generate_data(sequenceLen)\n",
    "            loss+=self.forward(xx,y)\n",
    "            self.backward()\n",
    "            self.update(eta=eta)\n",
    "            losses.append(loss)  \n",
    "            \n",
    "    return losses\n",
    "\n",
    "        \n",
    "Obj.train = train\n",
    "train32 = Obj()\n",
    "T,D,I,K = 1,1,32,1\n",
    "train32.init(D,I,K,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ts = [1,32,64]\n",
    "D,I,K = 1,32,1\n",
    "epochs = [550,2000,4100]\n",
    "etas = [0.01,0.1,0.6]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20,20]\n",
    "fig = plt.figure()\n",
    "fig,axs = plt.subplots(3,3)\n",
    "\n",
    "axs[1,0].set_title('Axis [1,0]')\n",
    "axs[1,1].set_title('Axis [1,1]')\n",
    "\n",
    "for T, i in zip(Ts, range(0,len(etas)*len(Ts),len(etas))):\n",
    "    for eta, j in zip(etas, range(len(etas))):\n",
    "        for epoch,k in zip(epochs, range(len(epochs))): \n",
    "            train32.init(D,I,K,T)\n",
    "            losses = train32.train(epochs=epoch, batchsize=1, eta=eta, sequenceLen=T)\n",
    "            \n",
    "            axs[i+j,0].set_title(f'T={T}, eta={eta} and {epoch}')\n",
    "            axs[i+j,0].set_xlabel('Iteration')\n",
    "            axs[i+j,0].set_ylabel('Loss')\n",
    "            \n",
    "            axs[i+j,1].set_title(f'T={T}, eta={eta} and {epoch}')\n",
    "            axs[i+j,1].set_xlabel('Iteration')\n",
    "            axs[i+j,1].set_ylabel('Loss')\n",
    "            \n",
    "            axs[i+j,2].set_title(f'T={T}, eta={eta} and {epoch}')\n",
    "            axs[i+j,2].set_xlabel('Iteration')\n",
    "            axs[i+j,2].set_ylabel('Loss')\n",
    "            \n",
    "            \n",
    "            \n",
    "            if epoch<epochs[-1]:\n",
    "                axs[i+j,1].plot(range(1,epoch+1),losses)\n",
    "                if epoch<epochs[1]:\n",
    "                    axs[i+j,0].plot(range(1,epoch+1),losses)\n",
    "                else:\n",
    "                    axs[i+j,2].plot(range(1,epoch+1),losses)\n",
    "            \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
